Author : Sarita,Sharmo,Ashish,Yogiraj
Assignment: 08 - Distributed EC2 Sorting
Date: 04/1/2016
JAVA File - SortNode.java
	    FileChunkLoader.java (retrieves data from s3- distributed)
	    TextSocket.java (path : socket/client/TextSocket/)
	    WebClient.java (path : socket/client/WebClient/)
	    WebServer.java (path : socket/client/WebServer/)
Bash Scripts -
	   start-cluster	  
	   stop-cluster
	   splitFileList.sh
	   dataShipper.sh
	   startsort.sh
	   execute.sh
           makessh.sh
	   server.sh
           startExportToS3.sh
           exportToS3.sh
           config -> file for AWS configuration
           sort
	   get-output
	   installjava.sh

Note : For below bash scripts , user have to individually enter the key value pair for automation
	startsort.sh
	startExportToS3.sh
	dataShipper.sh
	start-cluster.sh


Makefile - for AWS configuration and jar file creation
Prerequisites:
 Install the required softwares
 - Oracle JDK version 1.8
 - AWS CLI + Configuration

Make sure the following PATH variables are set before running:
- JAVA_HOME
- Also make sure that the aws configuration is done with required ssh setup done. Follow the script make ssh if the configuration is required

Directions to Execute:

Extract Awati_joshi_kalbhor_Sarkar_A8.tar.gz to folder Awati_joshi_kalbhor_Sarkar_A8 and navigate inside.
This folder contains the list of all the java files and the scripts along with the rule required for the entire implementation
The entire architecture is shown in the file architecture.jpeg
Architecture -> Architecture.jpg
REPORT : EC2_sorting_report
Required output file -> final.txt retrieved from the EC2 instance
Execution time for the sort job -> file name sort-time.txt which stores the sort time required to run the job


To run the entire flow of the code, follow the below instructions:
TO STEP UP CLUSTER ->
start-cluster {arg}
This script creates the required number of EC2 instances and saves the list of instances onto a local file, out.txt
Go through the Makefile to see the sequence of rules to be executed.

### AWS Configuration File 
############Common steps to be executed in order to generate the JAR file
• make client-server 
• make jar
#Cluster with 4 nodes started.
./start-cluster 4
./sort “Dry Bulb Temp” s3://cs6240sp16/climate s3://your-bucket/output
#After execution of the above command, the client at localhost collects output from each cluster and transfer to s3 bucket 	
./get-output -> This gives the required output file from EC2 instances to our local machine
./stop-cluster
#nodes stopped


NOTE: Final output present on the localhost as : final.txt
###############################################################################
Sample Sort Algoritm Approach : 

a) Consider p = number of EC2 instances. 
b) Each instances reads (total size of data from s3 bucket / p). The client ensures that each EC2 instance reads approximately equal amount of data. For the given input of total size 1.9 GB of data, each EC2 instance reads approximately 1 GB of data for clusters = 2 and 250 MB of data for clusters = 8.
c) The program reads one file at a time and store each record as a Record object. The entire data for the EC2 instance is stored in the Arraylist of Record objects. 
d) We sort the list of Records using Collections.Sort() , which internally uses quicksort. Sorting is done on “Dry Bulb Temp”
e) On the sorted list, we select (p-1) splitters (Dry Bulb Temp values) which are equidistant in the sorted list.
f) The splitters populated by the slaves are broadcasted to masters. 
g) Master node receives splitter list from all the slaves. It merges this received splitters with its own splitters.
h) Then these splitters (populated in the last step) are again sorted and new (p-1) splitters are selected which are equidistant in the sorted splitter list.
i) The new splitters generated by step (e) are broadcasted to all slaves.
j) The master node and slave nodes insert these new splitters into their existing sorted Record list (generated in step - d) using binary search.
k) Then Master as well as Slaves, distribute these buckets to appropriate nodes. This is many to many Node communication which is handled by acknowledgement logic (Discussed in the challenges).
l) Once each EC2 instance finishes receiving its buckets from all the nodes except itself, the received buckets are again sorted and written to output[clusterId].txt (eg : output1.txt)
m) When all slaves complete step – I, they send acknowledgement signal to Master that they completed sorting process. 
n) Once Master node receives Acknowledgement signal from all the slaves, the Master node sends “done” acknowledgement to the server running on master node.
o) After receiving the acknowledgement from all the slaves, the Master then communicates a ACK signal to the client which is then responsible for closing the TCP connection.
p) The final output i.e. top ten values of the required parameter “ dry bulb temp” is retrieved in the local file and the whole data file (sorted) is stored in the s3 bucket

##############################################################################
OUTPUT (Top 10 values) :
##############################################################################
Record [wban=26411, date=19980410, time=1153, temp=162.0]
Record [wban=03154, date=19991226, time=0755, temp=162.0]
Record [wban=13866, date=19990830, time=0054, temp=162.1]
Record [wban=93862, date=19990204, time=2150, temp=172.6]
Record [wban=14991, date=19970221, time=1051, temp=178.0]
Record [wban=03145, date=19991231, time=0255, temp=181.0]
Record [wban=40309, date=19960718, time=1553, temp=186.1]
Record [wban=03866, date=19991215, time=0955, temp=201.0]
Record [wban=14611, date=19991210, time=1855, temp=201.0]
Record [wban=14780, date=19991207, time=1155, temp=201.0]
###############################################################################

Time Taken to Sort data on M4.Large:
m4.2xlarge (26 ECUs, 8 vCPUs, 2.4 GHz, Intel Xeon E5-2676v3, 32 GiB memory, EBS only)

2 Instances : approx 45 minutes
8 Instances : approx 15 minutes

###############################################################################
CONCLUSION :

As per our analysis, using Sample Sort over very huge (~ 2GB compressed data) non-uniformly distributed data is more efficient and other Divide and Conquer algorithms. 
The reason is because of using p-1 (i.e. cluster count -1) buckets formed using splitters, which take care of the non-uniformity.
The data is divided among various node which imbibes parallelism into the process.
We have implemented the concept of synchronization using threads while EC2 communicates between each other to avoid data loss
As seen from the output, increasing number of EC2 instances (sort nodes) decreases speed of execution,
but that is not proportional since the data is stored in the heap for intermediate processing.
###############################################################################

REFERENCES :

www.stackoverflow.com
Class demo programs for the concept of join and the code snippet for Demo.scala and DemoReducer.java
http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html
http://www.tldp.org/LDP/abs/html/wrapper.html
http://linuxguru.org
http://parallelcomp.uw.hu/ch09lev1sec5.html
http://www3.cs.stonybrook.edu/~rezaul/Spring-2012/CSE613/CSE613-lecture-16.pdf
http://www.umiacs.umd.edu/research/EXPAR/papers/spaa96/node7.html
